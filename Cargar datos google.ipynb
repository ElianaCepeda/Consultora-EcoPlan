{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f44a612f-ac6f-495b-8124-b149b2934cad",
   "metadata": {},
   "source": [
    "# Carga de Edificios a MongoDB (Fuente de google)\n",
    "\n",
    "## Objetivo:\n",
    "Este notebook realiza el proceso de ETL para los datos de Google Open Buildings\n",
    "\n",
    "### Pasos del Proceso\n",
    "\n",
    "1.  **Extrae** las geometrías de los municipios PDET de MongoDB y los datos de los CSVs de Google.\n",
    "2.  **Transforma** los datos de edificios (CSV) al formato GeoJSON.\n",
    "3.  **Filtra** los edificios para quedarnos solo con los que están dentro de los polígonos PDET.\n",
    "4.  **Carga** los edificios filtrados a una nueva colección edificios en MongoDB."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b55fc93-085c-43db-bb7a-4bde2a2471d3",
   "metadata": {},
   "source": [
    "## Importar Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "954dc404-29a4-4a76-af60-5d662f0e91e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Librerías importadas correctamente.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "import json\n",
    "import os\n",
    "import glob  # Para encontrar archivos\n",
    "from shapely.wkt import loads  # Para leer las geometrías de Google\n",
    "from shapely.geometry import shape\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "print(\"Librerías importadas correctamente.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6527236-d5a6-41b9-a6a3-177326efb811",
   "metadata": {},
   "source": [
    "## Constantes y Conexión a MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "957d2a3d-71d7-43a7-aa2d-08736e7cb3d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡Conexión exitosa a MongoDB! BD: 'is394508_db'\n",
      "Leeremos de: 'municipios'\n",
      "Escribiremos en: 'edificios'\n"
     ]
    }
   ],
   "source": [
    "# --- Configuración de MongoDB ---\n",
    "CADENA_CONEXION_MONGO = \"mongodb://is394508:Y7hXfRv10UmhRPH@orion.javeriana.edu.co:27017/is394508_db?authSource=is394508_db\"\n",
    "NOMBRE_BASE_DATOS = \"is394508_db\"\n",
    "COLECCION_MUNICIPIOS = \"municipios\"  # Leemos de aquí\n",
    "COLECCION_EDIFICIOS = \"edificios\"    # Escribimos aquí\n",
    "\n",
    "# --- Configuración del Proceso ---\n",
    "# Leeremos los CSVs en trozos de 100,000 filas para no saturar la memoria\n",
    "TAMANO_CHUNK = 500\n",
    "\n",
    "# --- Conexión a MongoDB ---\n",
    "try:\n",
    "    client = MongoClient(CADENA_CONEXION_MONGO)\n",
    "    db = client[NOMBRE_BASE_DATOS]\n",
    "    collection_municipios = db[COLECCION_MUNICIPIOS]\n",
    "    collection_edificios = db[COLECCION_EDIFICIOS]\n",
    "    \n",
    "    client.admin.command('ping')\n",
    "    print(f\"¡Conexión exitosa a MongoDB! BD: '{NOMBRE_BASE_DATOS}'\")\n",
    "    print(f\"Leeremos de: '{COLECCION_MUNICIPIOS}'\")\n",
    "    print(f\"Escribiremos en: '{COLECCION_EDIFICIOS}'\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error conectando a MongoDB: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98af8deb-00fc-40af-822d-fe92eb0170b2",
   "metadata": {},
   "source": [
    "## Obtener Geometrías de Municipios PDET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d1ff110-3c5a-4d77-9380-bfd9c5d2e328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando la carga de municipios PDET desde MongoDB...\n",
      "Unificando todas las geometrías PDET en una sola (unary_union)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\elice\\AppData\\Local\\Temp\\ipykernel_16940\\2288795624.py:29: DeprecationWarning: The 'unary_union' attribute is deprecated, use the 'union_all()' method instead.\n",
      "  filtro_pdet_unificado = gdf_municipios_pdet.geometry.unary_union\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡Filtro PDET listo! (170 municipios cargados en 12.09s)\n"
     ]
    }
   ],
   "source": [
    "print(\"Iniciando la carga de municipios PDET desde MongoDB...\")\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    # Trae todos los documentos de la colección 'municipios'\n",
    "    cursor_municipios = collection_municipios.find({})\n",
    "    lista_municipios_docs = list(cursor_municipios)\n",
    "    \n",
    "    if not lista_municipios_docs:\n",
    "        print(\"¡ERROR! La colección 'municipios' está vacía. No se puede filtrar.\")\n",
    "        raise ValueError(\"No se encontraron municipios PDET.\")\n",
    "\n",
    "    # 1. Cargamos los documentos en un DataFrame de PANDAS normal\n",
    "    df_municipios = pd.DataFrame(lista_municipios_docs)\n",
    "\n",
    "    # 2. Convertimos manualmente nuestra columna 'geometria' (que son dicts) \n",
    "    #    a objetos de geometría de Shapely\n",
    "    geometrias_shapely = df_municipios['geometria'].apply(shape)\n",
    "\n",
    "    # 3. Creamos el GeoDataFrame usando el DataFrame y nuestras geometrías\n",
    "    gdf_municipios_pdet = gpd.GeoDataFrame(\n",
    "        df_municipios, \n",
    "        geometry=geometrias_shapely, \n",
    "        crs=\"EPSG:4326\"\n",
    "    )\n",
    "    \n",
    "    # 4. Optimizamos el filtro: Unimos todos los 170 polígonos en UNO SOLO\n",
    "    print(\"Unificando todas las geometrías PDET en una sola (unary_union)...\")\n",
    "    filtro_pdet_unificado = gdf_municipios_pdet.geometry.unary_union\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"¡Filtro PDET listo! ({len(lista_municipios_docs)} municipios cargados en {end_time - start_time:.2f}s)\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error cargando los municipios desde MongoDB: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24583a1a-b6e8-4b08-9557-750808af5009",
   "metadata": {},
   "source": [
    "## Preparar la Colección edificios_google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0973db2c-bf5f-4a89-aa5b-f91077d8b29d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparando la colección 'edificios'...\n",
      "Creando índice espacial '2dsphere' en el campo 'geometria'...\n",
      "¡Índice 2dsphere creado en 0.01s!\n"
     ]
    }
   ],
   "source": [
    "print(f\"Preparando la colección '{COLECCION_EDIFICIOS}'...\")\n",
    "\n",
    "# 1. Crear el índice espacial 2dsphere\n",
    "# Lo creamos ANTES de insertar los datos para que el proceso sea más eficiente.\n",
    "print(\"Creando índice espacial '2dsphere' en el campo 'geometria'...\")\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    collection_edificios.create_index([(\"geometria\", pymongo.GEOSPHERE)])\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"¡Índice 2dsphere creado en {end_time - start_time:.2f}s!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error creando el índice: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b65b84e-69ad-4145-8fc0-eaa4449a8cca",
   "metadata": {},
   "source": [
    "## Procesar los CSVs y Cargar a MongoDB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d189d35-1345-43d6-a2e6-498e832d1017",
   "metadata": {},
   "source": [
    "### Procesar archvio 1 (8df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59303f50-4b06-4983-b985-2e7470cba184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuración de Archivos Locales ---\n",
    "RUTA_CSV_INDIVIDUAL = r\"D:\\Javeriana\\Bases de datos\\proyecto\\csv google\\8df_buildings.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3bda064d-29ab-4d73-a688-3d15e2c8e11e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando el proceso de ETL para el archivo: D:\\Javeriana\\Bases de datos\\proyecto\\csv google\\8df_buildings.csv\n",
      "\n",
      "--- Procesando Archivo: 8df_buildings.csv ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebc894f82e3b465faafb620fbf85a8bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Progreso: 0 chunk [00:00, ? chunk/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- ¡PROCESO COMPLETO PARA ESTE ARCHIVO! ---\n",
      "Tiempo total: 0.01 minutos\n",
      "Total de filas CSV procesadas: 22806\n",
      "Total de edificios PDET cargados (en esta ejecución): 0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Iniciando el proceso de ETL para el archivo: {RUTA_CSV_INDIVIDUAL}\")\n",
    "\n",
    "# Contadores para el reporte final\n",
    "total_filas_procesadas = 0\n",
    "total_edificios_cargados = 0\n",
    "start_time_proceso = time.time()\n",
    "\n",
    "\n",
    "csv_file = RUTA_CSV_INDIVIDUAL\n",
    "print(f\"\\n--- Procesando Archivo: {os.path.basename(csv_file)} ---\")\n",
    "\n",
    "# Creamos el iterador de chunks\n",
    "chunk_iterator = pd.read_csv(csv_file, chunksize=TAMANO_CHUNK, encoding='latin-1', \n",
    "                             on_bad_lines='skip')\n",
    "\n",
    "# Envolvemos el iterador con TQDM.\n",
    "pbar = tqdm(chunk_iterator, desc=\"  Progreso\", unit=\" chunk\")\n",
    "\n",
    "# --- BUCLE Por cada \"trozo\" (chunk) ---\n",
    "for chunk_df in pbar:\n",
    "    \n",
    "    # 1. (T) Transformar: Convertir el texto WKT de Google a geometrías\n",
    "    geometrias = chunk_df['geometry'].apply(loads)\n",
    "    \n",
    "    # 2. (T) Transformar: Convertir el chunk de Pandas a un GeoDataFrame\n",
    "    gdf_chunk = gpd.GeoDataFrame(chunk_df, geometry=geometrias, crs=\"EPSG:4326\")\n",
    "    \n",
    "    # 3. (T) Filtrar: Usamos el filtro unificado para encontrar intersecciones\n",
    "    gdf_filtrado = gdf_chunk[gdf_chunk.geometry.intersects(filtro_pdet_unificado)]\n",
    "\n",
    "    total_filas_procesadas += len(chunk_df)\n",
    "    \n",
    "    # 4. (L) Cargar: Si encontramos edificios, los formateamos y cargamos\n",
    "    if not gdf_filtrado.empty:\n",
    "        \n",
    "        documentos_para_insertar = []\n",
    "        \n",
    "        # Formateamos al JSON que diseñamos en la Entrega 1\n",
    "        for _, edificio in gdf_filtrado.iterrows():\n",
    "            documento_json = {\n",
    "                \"fuente\": \"google\",\n",
    "                \"area_m2\": edificio['area_in_meters'],\n",
    "                \"confianza\": edificio['confidence'],\n",
    "                \"geometria\": edificio['geometry'].__geo_interface__\n",
    "            }\n",
    "            documentos_para_insertar.append(documento_json)\n",
    "        \n",
    "        # Insertar el lote de documentos\n",
    "        collection_edificios.insert_many(documentos_para_insertar)\n",
    "        \n",
    "        total_edificios_cargados += len(documentos_para_insertar)\n",
    "        \n",
    "        # Actualizamos el contador en la barra de progreso\n",
    "        pbar.set_postfix(edificios_cargados=total_edificios_cargados)\n",
    "        \n",
    "    else:\n",
    "        # Si no hay edificios, simplemente continuamos\n",
    "        pass\n",
    "    \n",
    "    # Liberar memoria explícitamente\n",
    "    del chunk_df, gdf_chunk, gdf_filtrado, geometrias\n",
    "\n",
    "end_time_proceso = time.time()\n",
    "print(\"\\n--- ¡PROCESO COMPLETO PARA ESTE ARCHIVO! ---\")\n",
    "print(f\"Tiempo total: {(end_time_proceso - start_time_proceso) / 60:.2f} minutos\")\n",
    "print(f\"Total de filas CSV procesadas: {total_filas_procesadas}\")\n",
    "print(f\"Total de edificios PDET cargados (en esta ejecución): {total_edificios_cargados}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58f3301-34af-487e-934e-20845ac21389",
   "metadata": {},
   "source": [
    "### Procesar archvio 2 (8e1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e17fc098-9d61-450d-9d10-68b8fe3b516c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuración de Archivos Locales ---\n",
    "RUTA_CSV_INDIVIDUAL = r\"D:\\Javeriana\\Bases de datos\\proyecto\\csv google\\8e1_buildings.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b459331d-a506-4891-bcae-7b9a2c75b7b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando el proceso de ETL para el archivo: D:\\Javeriana\\Bases de datos\\proyecto\\csv google\\8e1_buildings.csv\n",
      "\n",
      "--- Procesando Archivo: 8e1_buildings.csv ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a61f284221f49ef98c0c2249feef5db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Progreso: 0 chunk [00:00, ? chunk/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- ¡PROCESO COMPLETO PARA ESTE ARCHIVO! ---\n",
      "Tiempo total: 252.61 minutos\n",
      "Total de filas CSV procesadas: 277823\n",
      "Total de edificios PDET cargados (en esta ejecución): 80565\n"
     ]
    }
   ],
   "source": [
    "print(f\"Iniciando el proceso de ETL para el archivo: {RUTA_CSV_INDIVIDUAL}\")\n",
    "\n",
    "# Contadores para el reporte final\n",
    "total_filas_procesadas = 0\n",
    "total_edificios_cargados = 0\n",
    "start_time_proceso = time.time()\n",
    "\n",
    "\n",
    "csv_file = RUTA_CSV_INDIVIDUAL\n",
    "print(f\"\\n--- Procesando Archivo: {os.path.basename(csv_file)} ---\")\n",
    "\n",
    "# Creamos el iterador de chunks\n",
    "chunk_iterator = pd.read_csv(csv_file, chunksize=TAMANO_CHUNK, encoding='latin-1', \n",
    "                             on_bad_lines='skip')\n",
    "\n",
    "# Envolvemos el iterador con TQDM.\n",
    "pbar = tqdm(chunk_iterator, desc=\"  Progreso\", unit=\" chunk\")\n",
    "\n",
    "# --- BUCLE Por cada \"trozo\" (chunk) ---\n",
    "for chunk_df in pbar:\n",
    "    \n",
    "    # 1. (T) Transformar: Convertir el texto WKT de Google a geometrías\n",
    "    geometrias = chunk_df['geometry'].apply(loads)\n",
    "    \n",
    "    # 2. (T) Transformar: Convertir el chunk de Pandas a un GeoDataFrame\n",
    "    gdf_chunk = gpd.GeoDataFrame(chunk_df, geometry=geometrias, crs=\"EPSG:4326\")\n",
    "    \n",
    "    # 3. (T) Filtrar: Usamos el filtro unificado para encontrar intersecciones\n",
    "    gdf_filtrado = gdf_chunk[gdf_chunk.geometry.intersects(filtro_pdet_unificado)]\n",
    "\n",
    "    total_filas_procesadas += len(chunk_df)\n",
    "    \n",
    "    # 4. (L) Cargar: Si encontramos edificios, los formateamos y cargamos\n",
    "    if not gdf_filtrado.empty:\n",
    "        \n",
    "        documentos_para_insertar = []\n",
    "        \n",
    "        # Formateamos al JSON que diseñamos en la Entrega 1\n",
    "        for _, edificio in gdf_filtrado.iterrows():\n",
    "            documento_json = {\n",
    "                \"fuente\": \"google\",\n",
    "                \"area_m2\": edificio['area_in_meters'],\n",
    "                \"confianza\": edificio['confidence'],\n",
    "                \"geometria\": edificio['geometry'].__geo_interface__\n",
    "            }\n",
    "            documentos_para_insertar.append(documento_json)\n",
    "        \n",
    "        # Insertar el lote de documentos\n",
    "        collection_edificios.insert_many(documentos_para_insertar)\n",
    "        \n",
    "        total_edificios_cargados += len(documentos_para_insertar)\n",
    "        \n",
    "        # Actualizamos el contador en la barra de progreso\n",
    "        pbar.set_postfix(edificios_cargados=total_edificios_cargados)\n",
    "        \n",
    "    else:\n",
    "        # Si no hay edificios, simplemente continuamos\n",
    "        pass\n",
    "    \n",
    "    # Liberar memoria explícitamente\n",
    "    del chunk_df, gdf_chunk, gdf_filtrado, geometrias\n",
    "\n",
    "end_time_proceso = time.time()\n",
    "print(\"\\n--- ¡PROCESO COMPLETO PARA ESTE ARCHIVO! ---\")\n",
    "print(f\"Tiempo total: {(end_time_proceso - start_time_proceso) / 60:.2f} minutos\")\n",
    "print(f\"Total de filas CSV procesadas: {total_filas_procesadas}\")\n",
    "print(f\"Total de edificios PDET cargados (en esta ejecución): {total_edificios_cargados}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6e0e6b-7745-49cf-b11d-ab178d409cf7",
   "metadata": {},
   "source": [
    "### Procesar archvio 3 (8e3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b26e033a-3e51-45e1-a00b-79acbd5672a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuración de Archivos Locales ---\n",
    "RUTA_CSV_INDIVIDUAL = r\"D:\\Javeriana\\Bases de datos\\proyecto\\csv google\\8e3_buildings.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a67da576-34fe-43e7-9118-d1871df7572e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando el proceso de ETL para el archivo: D:\\Javeriana\\Bases de datos\\proyecto\\csv google\\8e3_buildings.csv\n",
      "\n",
      "--- Procesando Archivo: 8e3_buildings.csv ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff2a1cf9682c4f7993603c46bd1088c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Progreso: 0 chunk [00:00, ? chunk/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 29\u001b[0m\n\u001b[0;32m     26\u001b[0m gdf_chunk \u001b[38;5;241m=\u001b[39m gpd\u001b[38;5;241m.\u001b[39mGeoDataFrame(chunk_df, geometry\u001b[38;5;241m=\u001b[39mgeometrias, crs\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEPSG:4326\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# 3. (T) Filtrar: Usamos el filtro unificado para encontrar intersecciones\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m gdf_filtrado \u001b[38;5;241m=\u001b[39m gdf_chunk[gdf_chunk\u001b[38;5;241m.\u001b[39mgeometry\u001b[38;5;241m.\u001b[39mintersects(filtro_pdet_unificado)]\n\u001b[0;32m     31\u001b[0m total_filas_procesadas \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(chunk_df)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# 4. (L) Cargar: Si encontramos edificios, los formateamos y cargamos\u001b[39;00m\n",
      "File \u001b[1;32md:\\ProgramData\\anaconda3\\Lib\\site-packages\\geopandas\\base.py:3506\u001b[0m, in \u001b[0;36mGeoPandasBase.intersects\u001b[1;34m(self, other, align)\u001b[0m\n\u001b[0;32m   3397\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mintersects\u001b[39m(\u001b[38;5;28mself\u001b[39m, other, align\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   3398\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return a ``Series`` of ``dtype('bool')`` with value ``True`` for\u001b[39;00m\n\u001b[0;32m   3399\u001b[0m \u001b[38;5;124;03m    each aligned geometry that intersects `other`.\u001b[39;00m\n\u001b[0;32m   3400\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3504\u001b[0m \u001b[38;5;124;03m    GeoSeries.intersection\u001b[39;00m\n\u001b[0;32m   3505\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 3506\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _binary_op(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mintersects\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m, other, align)\n",
      "File \u001b[1;32md:\\ProgramData\\anaconda3\\Lib\\site-packages\\geopandas\\base.py:86\u001b[0m, in \u001b[0;36m_binary_op\u001b[1;34m(op, this, other, align, *args, **kwargs)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_binary_op\u001b[39m(op, this, other, align, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;66;03m# type: (str, GeoSeries, GeoSeries, args/kwargs) -> Series[bool/float]\u001b[39;00m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Binary operation on GeoSeries objects that returns a Series.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 86\u001b[0m     data, index \u001b[38;5;241m=\u001b[39m _delegate_binary_method(op, this, other, align, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Series(data, index\u001b[38;5;241m=\u001b[39mindex)\n",
      "File \u001b[1;32md:\\ProgramData\\anaconda3\\Lib\\site-packages\\geopandas\\base.py:70\u001b[0m, in \u001b[0;36m_delegate_binary_method\u001b[1;34m(op, this, other, align, *args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;28mtype\u001b[39m(this), \u001b[38;5;28mtype\u001b[39m(other))\n\u001b[1;32m---> 70\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(a_this, op)(other, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data, this\u001b[38;5;241m.\u001b[39mindex\n",
      "File \u001b[1;32md:\\ProgramData\\anaconda3\\Lib\\site-packages\\geopandas\\array.py:804\u001b[0m, in \u001b[0;36mGeometryArray.intersects\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    803\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mintersects\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[1;32m--> 804\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_binary_method(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mintersects\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m, other)\n",
      "File \u001b[1;32md:\\ProgramData\\anaconda3\\Lib\\site-packages\\geopandas\\array.py:780\u001b[0m, in \u001b[0;36mGeometryArray._binary_method\u001b[1;34m(op, left, right, **kwargs)\u001b[0m\n\u001b[0;32m    777\u001b[0m         _crs_mismatch_warn(left, right, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m7\u001b[39m)\n\u001b[0;32m    778\u001b[0m     right \u001b[38;5;241m=\u001b[39m right\u001b[38;5;241m.\u001b[39m_data\n\u001b[1;32m--> 780\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(shapely, op)(left\u001b[38;5;241m.\u001b[39m_data, right, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\ProgramData\\anaconda3\\Lib\\site-packages\\shapely\\decorators.py:77\u001b[0m, in \u001b[0;36mmultithreading_enabled.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m arr \u001b[38;5;129;01min\u001b[39;00m array_args:\n\u001b[0;32m     76\u001b[0m         arr\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mwriteable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m---> 77\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m arr, old_flag \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(array_args, old_flags):\n",
      "File \u001b[1;32md:\\ProgramData\\anaconda3\\Lib\\site-packages\\shapely\\predicates.py:778\u001b[0m, in \u001b[0;36mintersects\u001b[1;34m(a, b, **kwargs)\u001b[0m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;129m@multithreading_enabled\u001b[39m\n\u001b[0;32m    748\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mintersects\u001b[39m(a, b, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    749\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns True if A and B share any portion of space.\u001b[39;00m\n\u001b[0;32m    750\u001b[0m \n\u001b[0;32m    751\u001b[0m \u001b[38;5;124;03m    Intersects implies that overlaps, touches, covers, or within are True.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    776\u001b[0m \u001b[38;5;124;03m    False\u001b[39;00m\n\u001b[0;32m    777\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 778\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mintersects(a, b, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(f\"Iniciando el proceso de ETL para el archivo: {RUTA_CSV_INDIVIDUAL}\")\n",
    "\n",
    "# Contadores para el reporte final\n",
    "total_filas_procesadas = 0\n",
    "total_edificios_cargados = 0\n",
    "start_time_proceso = time.time()\n",
    "\n",
    "\n",
    "csv_file = RUTA_CSV_INDIVIDUAL\n",
    "print(f\"\\n--- Procesando Archivo: {os.path.basename(csv_file)} ---\")\n",
    "\n",
    "# Creamos el iterador de chunks\n",
    "chunk_iterator = pd.read_csv(csv_file, chunksize=TAMANO_CHUNK, encoding='latin-1', \n",
    "                             on_bad_lines='skip')\n",
    "\n",
    "# Envolvemos el iterador con TQDM.\n",
    "pbar = tqdm(chunk_iterator, desc=\"  Progreso\", unit=\" chunk\")\n",
    "\n",
    "# --- BUCLE Por cada \"trozo\" (chunk) ---\n",
    "for chunk_df in pbar:\n",
    "    \n",
    "    # 1. (T) Transformar: Convertir el texto WKT de Google a geometrías\n",
    "    geometrias = chunk_df['geometry'].apply(loads)\n",
    "    \n",
    "    # 2. (T) Transformar: Convertir el chunk de Pandas a un GeoDataFrame\n",
    "    gdf_chunk = gpd.GeoDataFrame(chunk_df, geometry=geometrias, crs=\"EPSG:4326\")\n",
    "    \n",
    "    # 3. (T) Filtrar: Usamos el filtro unificado para encontrar intersecciones\n",
    "    gdf_filtrado = gdf_chunk[gdf_chunk.geometry.intersects(filtro_pdet_unificado)]\n",
    "\n",
    "    total_filas_procesadas += len(chunk_df)\n",
    "    \n",
    "    # 4. (L) Cargar: Si encontramos edificios, los formateamos y cargamos\n",
    "    if not gdf_filtrado.empty:\n",
    "        \n",
    "        documentos_para_insertar = []\n",
    "        \n",
    "        # Formateamos al JSON que diseñamos en la Entrega 1\n",
    "        for _, edificio in gdf_filtrado.iterrows():\n",
    "            documento_json = {\n",
    "                \"fuente\": \"google\",\n",
    "                \"area_m2\": edificio['area_in_meters'],\n",
    "                \"confianza\": edificio['confidence'],\n",
    "                \"geometria\": edificio['geometry'].__geo_interface__\n",
    "            }\n",
    "            documentos_para_insertar.append(documento_json)\n",
    "        \n",
    "        # Insertar el lote de documentos\n",
    "        collection_edificios.insert_many(documentos_para_insertar)\n",
    "        \n",
    "        total_edificios_cargados += len(documentos_para_insertar)\n",
    "        \n",
    "        # Actualizamos el contador en la barra de progreso\n",
    "        pbar.set_postfix(edificios_cargados=total_edificios_cargados)\n",
    "        \n",
    "    else:\n",
    "        # Si no hay edificios, simplemente continuamos\n",
    "        pass\n",
    "    \n",
    "    # Liberar memoria explícitamente\n",
    "    del chunk_df, gdf_chunk, gdf_filtrado, geometrias\n",
    "\n",
    "end_time_proceso = time.time()\n",
    "print(\"\\n--- ¡PROCESO COMPLETO PARA ESTE ARCHIVO! ---\")\n",
    "print(f\"Tiempo total: {(end_time_proceso - start_time_proceso) / 60:.2f} minutos\")\n",
    "print(f\"Total de filas CSV procesadas: {total_filas_procesadas}\")\n",
    "print(f\"Total de edificios PDET cargados (en esta ejecución): {total_edificios_cargados}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7efd914-ce75-4a59-9e59-696bf2691b10",
   "metadata": {},
   "source": [
    "### Procesar archvio 4 (8e5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778c6289-54f9-4d8f-9bda-be6fe7a11657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuración de Archivos Locales ---\n",
    "RUTA_CSV_INDIVIDUAL = r\"D:\\Javeriana\\Bases de datos\\proyecto\\csv google\\8e5_buildings.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45409aaf-5120-470e-9985-0fc20cf7c199",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Iniciando el proceso de ETL para el archivo: {RUTA_CSV_INDIVIDUAL}\")\n",
    "\n",
    "# Contadores para el reporte final\n",
    "total_filas_procesadas = 0\n",
    "total_edificios_cargados = 0\n",
    "start_time_proceso = time.time()\n",
    "\n",
    "\n",
    "csv_file = RUTA_CSV_INDIVIDUAL\n",
    "print(f\"\\n--- Procesando Archivo: {os.path.basename(csv_file)} ---\")\n",
    "\n",
    "# Creamos el iterador de chunks\n",
    "chunk_iterator = pd.read_csv(csv_file, chunksize=TAMANO_CHUNK, encoding='latin-1', \n",
    "                             on_bad_lines='skip')\n",
    "\n",
    "# Envolvemos el iterador con TQDM.\n",
    "pbar = tqdm(chunk_iterator, desc=\"  Progreso\", unit=\" chunk\")\n",
    "\n",
    "# --- BUCLE Por cada \"trozo\" (chunk) ---\n",
    "for chunk_df in pbar:\n",
    "    \n",
    "    # 1. (T) Transformar: Convertir el texto WKT de Google a geometrías\n",
    "    geometrias = chunk_df['geometry'].apply(loads)\n",
    "    \n",
    "    # 2. (T) Transformar: Convertir el chunk de Pandas a un GeoDataFrame\n",
    "    gdf_chunk = gpd.GeoDataFrame(chunk_df, geometry=geometrias, crs=\"EPSG:4326\")\n",
    "    \n",
    "    # 3. (T) Filtrar: Usamos el filtro unificado para encontrar intersecciones\n",
    "    gdf_filtrado = gdf_chunk[gdf_chunk.geometry.intersects(filtro_pdet_unificado)]\n",
    "\n",
    "    total_filas_procesadas += len(chunk_df)\n",
    "    \n",
    "    # 4. (L) Cargar: Si encontramos edificios, los formateamos y cargamos\n",
    "    if not gdf_filtrado.empty:\n",
    "        \n",
    "        documentos_para_insertar = []\n",
    "        \n",
    "        # Formateamos al JSON que diseñamos en la Entrega 1\n",
    "        for _, edificio in gdf_filtrado.iterrows():\n",
    "            documento_json = {\n",
    "                \"fuente\": \"google\",\n",
    "                \"area_m2\": edificio['area_in_meters'],\n",
    "                \"confianza\": edificio['confidence'],\n",
    "                \"geometria\": edificio['geometry'].__geo_interface__\n",
    "            }\n",
    "            documentos_para_insertar.append(documento_json)\n",
    "        \n",
    "        # Insertar el lote de documentos\n",
    "        collection_edificios.insert_many(documentos_para_insertar)\n",
    "        \n",
    "        total_edificios_cargados += len(documentos_para_insertar)\n",
    "        \n",
    "        # Actualizamos el contador en la barra de progreso\n",
    "        pbar.set_postfix(edificios_cargados=total_edificios_cargados)\n",
    "        \n",
    "    else:\n",
    "        # Si no hay edificios, simplemente continuamos\n",
    "        pass\n",
    "    \n",
    "    # Liberar memoria explícitamente\n",
    "    del chunk_df, gdf_chunk, gdf_filtrado, geometrias\n",
    "\n",
    "end_time_proceso = time.time()\n",
    "print(\"\\n--- ¡PROCESO COMPLETO PARA ESTE ARCHIVO! ---\")\n",
    "print(f\"Tiempo total: {(end_time_proceso - start_time_proceso) / 60:.2f} minutos\")\n",
    "print(f\"Total de filas CSV procesadas: {total_filas_procesadas}\")\n",
    "print(f\"Total de edificios PDET cargados (en esta ejecución): {total_edificios_cargados}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645738ee-3933-42d7-b260-e7f042cbc877",
   "metadata": {},
   "source": [
    "### Procesar archvio 5 (8e7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8c27fc-56fa-4ee7-95dc-300c2ef37834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuración de Archivos Locales ---\n",
    "RUTA_CSV_INDIVIDUAL = r\"D:\\Javeriana\\Bases de datos\\proyecto\\csv google\\8e7_buildings.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143a926b-d6eb-4db5-9886-2c3a3cb238e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Iniciando el proceso de ETL para el archivo: {RUTA_CSV_INDIVIDUAL}\")\n",
    "\n",
    "# Contadores para el reporte final\n",
    "total_filas_procesadas = 0\n",
    "total_edificios_cargados = 0\n",
    "start_time_proceso = time.time()\n",
    "\n",
    "\n",
    "csv_file = RUTA_CSV_INDIVIDUAL\n",
    "print(f\"\\n--- Procesando Archivo: {os.path.basename(csv_file)} ---\")\n",
    "\n",
    "# Creamos el iterador de chunks\n",
    "chunk_iterator = pd.read_csv(csv_file, chunksize=TAMANO_CHUNK, encoding='latin-1', \n",
    "                             on_bad_lines='skip')\n",
    "\n",
    "# Envolvemos el iterador con TQDM.\n",
    "pbar = tqdm(chunk_iterator, desc=\"  Progreso\", unit=\" chunk\")\n",
    "\n",
    "# --- BUCLE Por cada \"trozo\" (chunk) ---\n",
    "for chunk_df in pbar:\n",
    "    \n",
    "    # 1. (T) Transformar: Convertir el texto WKT de Google a geometrías\n",
    "    geometrias = chunk_df['geometry'].apply(loads)\n",
    "    \n",
    "    # 2. (T) Transformar: Convertir el chunk de Pandas a un GeoDataFrame\n",
    "    gdf_chunk = gpd.GeoDataFrame(chunk_df, geometry=geometrias, crs=\"EPSG:4326\")\n",
    "    \n",
    "    # 3. (T) Filtrar: Usamos el filtro unificado para encontrar intersecciones\n",
    "    gdf_filtrado = gdf_chunk[gdf_chunk.geometry.intersects(filtro_pdet_unificado)]\n",
    "\n",
    "    total_filas_procesadas += len(chunk_df)\n",
    "    \n",
    "    # 4. (L) Cargar: Si encontramos edificios, los formateamos y cargamos\n",
    "    if not gdf_filtrado.empty:\n",
    "        \n",
    "        documentos_para_insertar = []\n",
    "        \n",
    "        # Formateamos al JSON que diseñamos en la Entrega 1\n",
    "        for _, edificio in gdf_filtrado.iterrows():\n",
    "            documento_json = {\n",
    "                \"fuente\": \"google\",\n",
    "                \"area_m2\": edificio['area_in_meters'],\n",
    "                \"confianza\": edificio['confidence'],\n",
    "                \"geometria\": edificio['geometry'].__geo_interface__\n",
    "            }\n",
    "            documentos_para_insertar.append(documento_json)\n",
    "        \n",
    "        # Insertar el lote de documentos\n",
    "        collection_edificios.insert_many(documentos_para_insertar)\n",
    "        \n",
    "        total_edificios_cargados += len(documentos_para_insertar)\n",
    "        \n",
    "        # Actualizamos el contador en la barra de progreso\n",
    "        pbar.set_postfix(edificios_cargados=total_edificios_cargados)\n",
    "        \n",
    "    else:\n",
    "        # Si no hay edificios, simplemente continuamos\n",
    "        pass\n",
    "    \n",
    "    # Liberar memoria explícitamente\n",
    "    del chunk_df, gdf_chunk, gdf_filtrado, geometrias\n",
    "\n",
    "end_time_proceso = time.time()\n",
    "print(\"\\n--- ¡PROCESO COMPLETO PARA ESTE ARCHIVO! ---\")\n",
    "print(f\"Tiempo total: {(end_time_proceso - start_time_proceso) / 60:.2f} minutos\")\n",
    "print(f\"Total de filas CSV procesadas: {total_filas_procesadas}\")\n",
    "print(f\"Total de edificios PDET cargados (en esta ejecución): {total_edificios_cargados}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96973897-4809-4147-b889-1fb5e9245780",
   "metadata": {},
   "source": [
    "### Procesar archvio 6 (8e9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c95d3ee-6cff-49d4-8f4d-eae5e62e4f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuración de Archivos Locales ---\n",
    "RUTA_CSV_INDIVIDUAL = r\"D:\\Javeriana\\Bases de datos\\proyecto\\csv google\\8e9_buildings.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad591f36-872b-4c38-b642-fd52edd50b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Iniciando el proceso de ETL para el archivo: {RUTA_CSV_INDIVIDUAL}\")\n",
    "\n",
    "# Contadores para el reporte final\n",
    "total_filas_procesadas = 0\n",
    "total_edificios_cargados = 0\n",
    "start_time_proceso = time.time()\n",
    "\n",
    "\n",
    "csv_file = RUTA_CSV_INDIVIDUAL\n",
    "print(f\"\\n--- Procesando Archivo: {os.path.basename(csv_file)} ---\")\n",
    "\n",
    "# Creamos el iterador de chunks\n",
    "chunk_iterator = pd.read_csv(csv_file, chunksize=TAMANO_CHUNK, encoding='latin-1', \n",
    "                             on_bad_lines='skip')\n",
    "\n",
    "# Envolvemos el iterador con TQDM.\n",
    "pbar = tqdm(chunk_iterator, desc=\"  Progreso\", unit=\" chunk\")\n",
    "\n",
    "# --- BUCLE Por cada \"trozo\" (chunk) ---\n",
    "for chunk_df in pbar:\n",
    "    \n",
    "    # 1. (T) Transformar: Convertir el texto WKT de Google a geometrías\n",
    "    geometrias = chunk_df['geometry'].apply(loads)\n",
    "    \n",
    "    # 2. (T) Transformar: Convertir el chunk de Pandas a un GeoDataFrame\n",
    "    gdf_chunk = gpd.GeoDataFrame(chunk_df, geometry=geometrias, crs=\"EPSG:4326\")\n",
    "    \n",
    "    # 3. (T) Filtrar: Usamos el filtro unificado para encontrar intersecciones\n",
    "    gdf_filtrado = gdf_chunk[gdf_chunk.geometry.intersects(filtro_pdet_unificado)]\n",
    "\n",
    "    total_filas_procesadas += len(chunk_df)\n",
    "    \n",
    "    # 4. (L) Cargar: Si encontramos edificios, los formateamos y cargamos\n",
    "    if not gdf_filtrado.empty:\n",
    "        \n",
    "        documentos_para_insertar = []\n",
    "        \n",
    "        # Formateamos al JSON que diseñamos en la Entrega 1\n",
    "        for _, edificio in gdf_filtrado.iterrows():\n",
    "            documento_json = {\n",
    "                \"fuente\": \"google\",\n",
    "                \"area_m2\": edificio['area_in_meters'],\n",
    "                \"confianza\": edificio['confidence'],\n",
    "                \"geometria\": edificio['geometry'].__geo_interface__\n",
    "            }\n",
    "            documentos_para_insertar.append(documento_json)\n",
    "        \n",
    "        # Insertar el lote de documentos\n",
    "        collection_edificios.insert_many(documentos_para_insertar)\n",
    "        \n",
    "        total_edificios_cargados += len(documentos_para_insertar)\n",
    "        \n",
    "        # Actualizamos el contador en la barra de progreso\n",
    "        pbar.set_postfix(edificios_cargados=total_edificios_cargados)\n",
    "        \n",
    "    else:\n",
    "        # Si no hay edificios, simplemente continuamos\n",
    "        pass\n",
    "    \n",
    "    # Liberar memoria explícitamente\n",
    "    del chunk_df, gdf_chunk, gdf_filtrado, geometrias\n",
    "\n",
    "end_time_proceso = time.time()\n",
    "print(\"\\n--- ¡PROCESO COMPLETO PARA ESTE ARCHIVO! ---\")\n",
    "print(f\"Tiempo total: {(end_time_proceso - start_time_proceso) / 60:.2f} minutos\")\n",
    "print(f\"Total de filas CSV procesadas: {total_filas_procesadas}\")\n",
    "print(f\"Total de edificios PDET cargados (en esta ejecución): {total_edificios_cargados}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baed87ab-fc4d-42be-87dc-32dc26a23806",
   "metadata": {},
   "source": [
    "### Procesar archvio 7 (8ef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54cde49d-1595-4673-ba59-1828c96f9cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuración de Archivos Locales ---\n",
    "RUTA_CSV_INDIVIDUAL = r\"D:\\Javeriana\\Bases de datos\\proyecto\\csv google\\8ef_buildings.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78538dc4-7551-4d61-ad80-a25bdd59f631",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Iniciando el proceso de ETL para el archivo: {RUTA_CSV_INDIVIDUAL}\")\n",
    "\n",
    "# Contadores para el reporte final\n",
    "total_filas_procesadas = 0\n",
    "total_edificios_cargados = 0\n",
    "start_time_proceso = time.time()\n",
    "\n",
    "\n",
    "csv_file = RUTA_CSV_INDIVIDUAL\n",
    "print(f\"\\n--- Procesando Archivo: {os.path.basename(csv_file)} ---\")\n",
    "\n",
    "# Creamos el iterador de chunks\n",
    "chunk_iterator = pd.read_csv(csv_file, chunksize=TAMANO_CHUNK, encoding='latin-1', \n",
    "                             on_bad_lines='skip')\n",
    "\n",
    "# Envolvemos el iterador con TQDM.\n",
    "pbar = tqdm(chunk_iterator, desc=\"  Progreso\", unit=\" chunk\")\n",
    "\n",
    "# --- BUCLE Por cada \"trozo\" (chunk) ---\n",
    "for chunk_df in pbar:\n",
    "    \n",
    "    # 1. (T) Transformar: Convertir el texto WKT de Google a geometrías\n",
    "    geometrias = chunk_df['geometry'].apply(loads)\n",
    "    \n",
    "    # 2. (T) Transformar: Convertir el chunk de Pandas a un GeoDataFrame\n",
    "    gdf_chunk = gpd.GeoDataFrame(chunk_df, geometry=geometrias, crs=\"EPSG:4326\")\n",
    "    \n",
    "    # 3. (T) Filtrar: Usamos el filtro unificado para encontrar intersecciones\n",
    "    gdf_filtrado = gdf_chunk[gdf_chunk.geometry.intersects(filtro_pdet_unificado)]\n",
    "\n",
    "    total_filas_procesadas += len(chunk_df)\n",
    "    \n",
    "    # 4. (L) Cargar: Si encontramos edificios, los formateamos y cargamos\n",
    "    if not gdf_filtrado.empty:\n",
    "        \n",
    "        documentos_para_insertar = []\n",
    "        \n",
    "        # Formateamos al JSON que diseñamos en la Entrega 1\n",
    "        for _, edificio in gdf_filtrado.iterrows():\n",
    "            documento_json = {\n",
    "                \"fuente\": \"google\",\n",
    "                \"area_m2\": edificio['area_in_meters'],\n",
    "                \"confianza\": edificio['confidence'],\n",
    "                \"geometria\": edificio['geometry'].__geo_interface__\n",
    "            }\n",
    "            documentos_para_insertar.append(documento_json)\n",
    "        \n",
    "        # Insertar el lote de documentos\n",
    "        collection_edificios.insert_many(documentos_para_insertar)\n",
    "        \n",
    "        total_edificios_cargados += len(documentos_para_insertar)\n",
    "        \n",
    "        # Actualizamos el contador en la barra de progreso\n",
    "        pbar.set_postfix(edificios_cargados=total_edificios_cargados)\n",
    "        \n",
    "    else:\n",
    "        # Si no hay edificios, simplemente continuamos\n",
    "        pass\n",
    "    \n",
    "    # Liberar memoria explícitamente\n",
    "    del chunk_df, gdf_chunk, gdf_filtrado, geometrias\n",
    "\n",
    "end_time_proceso = time.time()\n",
    "print(\"\\n--- ¡PROCESO COMPLETO PARA ESTE ARCHIVO! ---\")\n",
    "print(f\"Tiempo total: {(end_time_proceso - start_time_proceso) / 60:.2f} minutos\")\n",
    "print(f\"Total de filas CSV procesadas: {total_filas_procesadas}\")\n",
    "print(f\"Total de edificios PDET cargados (en esta ejecución): {total_edificios_cargados}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96eeaf0-2e01-4099-90e2-9f966d8bfcd4",
   "metadata": {},
   "source": [
    "### Procesar archvio 8 (91d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b7ecd6-ffbf-403c-835f-a54fbaee902d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuración de Archivos Locales ---\n",
    "RUTA_CSV_INDIVIDUAL = r\"D:\\Javeriana\\Bases de datos\\proyecto\\csv google\\91d_buildings.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2dc23dd-f66f-4e7f-97ad-ca83bb28f8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Iniciando el proceso de ETL para el archivo: {RUTA_CSV_INDIVIDUAL}\")\n",
    "\n",
    "# Contadores para el reporte final\n",
    "total_filas_procesadas = 0\n",
    "total_edificios_cargados = 0\n",
    "start_time_proceso = time.time()\n",
    "\n",
    "\n",
    "csv_file = RUTA_CSV_INDIVIDUAL\n",
    "print(f\"\\n--- Procesando Archivo: {os.path.basename(csv_file)} ---\")\n",
    "\n",
    "# Creamos el iterador de chunks\n",
    "chunk_iterator = pd.read_csv(csv_file, chunksize=TAMANO_CHUNK, encoding='latin-1', \n",
    "                             on_bad_lines='skip')\n",
    "\n",
    "# Envolvemos el iterador con TQDM.\n",
    "pbar = tqdm(chunk_iterator, desc=\"  Progreso\", unit=\" chunk\")\n",
    "\n",
    "# --- BUCLE Por cada \"trozo\" (chunk) ---\n",
    "for chunk_df in pbar:\n",
    "    \n",
    "    # 1. (T) Transformar: Convertir el texto WKT de Google a geometrías\n",
    "    geometrias = chunk_df['geometry'].apply(loads)\n",
    "    \n",
    "    # 2. (T) Transformar: Convertir el chunk de Pandas a un GeoDataFrame\n",
    "    gdf_chunk = gpd.GeoDataFrame(chunk_df, geometry=geometrias, crs=\"EPSG:4326\")\n",
    "    \n",
    "    # 3. (T) Filtrar: Usamos el filtro unificado para encontrar intersecciones\n",
    "    gdf_filtrado = gdf_chunk[gdf_chunk.geometry.intersects(filtro_pdet_unificado)]\n",
    "\n",
    "    total_filas_procesadas += len(chunk_df)\n",
    "    \n",
    "    # 4. (L) Cargar: Si encontramos edificios, los formateamos y cargamos\n",
    "    if not gdf_filtrado.empty:\n",
    "        \n",
    "        documentos_para_insertar = []\n",
    "        \n",
    "        # Formateamos al JSON que diseñamos en la Entrega 1\n",
    "        for _, edificio in gdf_filtrado.iterrows():\n",
    "            documento_json = {\n",
    "                \"fuente\": \"google\",\n",
    "                \"area_m2\": edificio['area_in_meters'],\n",
    "                \"confianza\": edificio['confidence'],\n",
    "                \"geometria\": edificio['geometry'].__geo_interface__\n",
    "            }\n",
    "            documentos_para_insertar.append(documento_json)\n",
    "        \n",
    "        # Insertar el lote de documentos\n",
    "        collection_edificios.insert_many(documentos_para_insertar)\n",
    "        \n",
    "        total_edificios_cargados += len(documentos_para_insertar)\n",
    "        \n",
    "        # Actualizamos el contador en la barra de progreso\n",
    "        pbar.set_postfix(edificios_cargados=total_edificios_cargados)\n",
    "        \n",
    "    else:\n",
    "        # Si no hay edificios, simplemente continuamos\n",
    "        pass\n",
    "    \n",
    "    # Liberar memoria explícitamente\n",
    "    del chunk_df, gdf_chunk, gdf_filtrado, geometrias\n",
    "\n",
    "end_time_proceso = time.time()\n",
    "print(\"\\n--- ¡PROCESO COMPLETO PARA ESTE ARCHIVO! ---\")\n",
    "print(f\"Tiempo total: {(end_time_proceso - start_time_proceso) / 60:.2f} minutos\")\n",
    "print(f\"Total de filas CSV procesadas: {total_filas_procesadas}\")\n",
    "print(f\"Total de edificios PDET cargados (en esta ejecución): {total_edificios_cargados}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdec3617-77aa-4b53-a908-9fcc6595b130",
   "metadata": {},
   "source": [
    "### Procesar archvio 9 (91f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7278b286-fb25-45ae-9841-b813513b71b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuración de Archivos Locales ---\n",
    "RUTA_CSV_INDIVIDUAL = r\"D:\\Javeriana\\Bases de datos\\proyecto\\csv google\\91f_buildings.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e5de85-dd44-4746-a060-d44d59fa4331",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Iniciando el proceso de ETL para el archivo: {RUTA_CSV_INDIVIDUAL}\")\n",
    "\n",
    "# Contadores para el reporte final\n",
    "total_filas_procesadas = 0\n",
    "total_edificios_cargados = 0\n",
    "start_time_proceso = time.time()\n",
    "\n",
    "\n",
    "csv_file = RUTA_CSV_INDIVIDUAL\n",
    "print(f\"\\n--- Procesando Archivo: {os.path.basename(csv_file)} ---\")\n",
    "\n",
    "# Creamos el iterador de chunks\n",
    "chunk_iterator = pd.read_csv(csv_file, chunksize=TAMANO_CHUNK, encoding='latin-1', \n",
    "                             on_bad_lines='skip')\n",
    "\n",
    "# Envolvemos el iterador con TQDM.\n",
    "pbar = tqdm(chunk_iterator, desc=\"  Progreso\", unit=\" chunk\")\n",
    "\n",
    "# --- BUCLE Por cada \"trozo\" (chunk) ---\n",
    "for chunk_df in pbar:\n",
    "    \n",
    "    # 1. (T) Transformar: Convertir el texto WKT de Google a geometrías\n",
    "    geometrias = chunk_df['geometry'].apply(loads)\n",
    "    \n",
    "    # 2. (T) Transformar: Convertir el chunk de Pandas a un GeoDataFrame\n",
    "    gdf_chunk = gpd.GeoDataFrame(chunk_df, geometry=geometrias, crs=\"EPSG:4326\")\n",
    "    \n",
    "    # 3. (T) Filtrar: Usamos el filtro unificado para encontrar intersecciones\n",
    "    gdf_filtrado = gdf_chunk[gdf_chunk.geometry.intersects(filtro_pdet_unificado)]\n",
    "\n",
    "    total_filas_procesadas += len(chunk_df)\n",
    "    \n",
    "    # 4. (L) Cargar: Si encontramos edificios, los formateamos y cargamos\n",
    "    if not gdf_filtrado.empty:\n",
    "        \n",
    "        documentos_para_insertar = []\n",
    "        \n",
    "        # Formateamos al JSON que diseñamos en la Entrega 1\n",
    "        for _, edificio in gdf_filtrado.iterrows():\n",
    "            documento_json = {\n",
    "                \"fuente\": \"google\",\n",
    "                \"area_m2\": edificio['area_in_meters'],\n",
    "                \"confianza\": edificio['confidence'],\n",
    "                \"geometria\": edificio['geometry'].__geo_interface__\n",
    "            }\n",
    "            documentos_para_insertar.append(documento_json)\n",
    "        \n",
    "        # Insertar el lote de documentos\n",
    "        collection_edificios.insert_many(documentos_para_insertar)\n",
    "        \n",
    "        total_edificios_cargados += len(documentos_para_insertar)\n",
    "        \n",
    "        # Actualizamos el contador en la barra de progreso\n",
    "        pbar.set_postfix(edificios_cargados=total_edificios_cargados)\n",
    "        \n",
    "    else:\n",
    "        # Si no hay edificios, simplemente continuamos\n",
    "        pass\n",
    "    \n",
    "    # Liberar memoria explícitamente\n",
    "    del chunk_df, gdf_chunk, gdf_filtrado, geometrias\n",
    "\n",
    "end_time_proceso = time.time()\n",
    "print(\"\\n--- ¡PROCESO COMPLETO PARA ESTE ARCHIVO! ---\")\n",
    "print(f\"Tiempo total: {(end_time_proceso - start_time_proceso) / 60:.2f} minutos\")\n",
    "print(f\"Total de filas CSV procesadas: {total_filas_procesadas}\")\n",
    "print(f\"Total de edificios PDET cargados (en esta ejecución): {total_edificios_cargados}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f49a1f-f8c6-42db-b75e-f2e63511d567",
   "metadata": {},
   "source": [
    "## Verificación Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0219612b-6e06-4347-9177-d0334f8f6538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cuenta el número total de documentos en la colección\n",
    "conteo_documentos = collection_edificios.count_documents({})\n",
    "print(f\"Verificación: La colección '{COLECCION_EDIFICIOS}' ahora tiene {conteo_documentos} documentos.\")\n",
    "\n",
    "# Recupera y muestra un documento de ejemplo\n",
    "print(\"\\nDocumento de ejemplo recuperado de MongoDB:\")\n",
    "documento_ejemplo = collection_edificios.find_one()\n",
    "if documento_ejemplo:\n",
    "    # Quitamos la geometría y el _id para que sea más fácil de leer\n",
    "    documento_ejemplo.pop(\"geometria\", None) \n",
    "    documento_ejemplo.pop(\"_id\", None) \n",
    "    print(json.dumps(documento_ejemplo, indent=2, ensure_ascii=False))\n",
    "else:\n",
    "    print(\"No se pudo recuperar un documento (la colección puede estar vacía si no se encontraron edificios).\")\n",
    "\n",
    "client.close()\n",
    "print(\"\\nConexión a MongoDB cerrada.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
